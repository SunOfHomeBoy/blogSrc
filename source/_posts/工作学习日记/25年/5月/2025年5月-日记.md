---
title: 2025年5月 日记
date: 2025年5月7日 16:59:38
update: 
tags: 日记
categories: Diary
---

# 2025年5月7日 周三
  ## [【简史三部曲】人类简史+未来简史+今日简史（尤瓦尔·赫拉利代表作）](https://www.bilibili.com/video/BV1SC5HzYExM?spm_id_from=333.788.player.switch&vd_source=3a9354daab34de3be4d6748478d38049&p=7)

  ## 要闻
  + 克什米尔 印巴冲突
  + 金价暴涨
  + 习近平 访问红场

  ## 更新AI编辑器规则
  [项目规则](../Trae_MCP_Agent/project_rules.md)
  [用户规则](../Trae_MCP_Agent/user_rules.md)

# 2025年5月8日 周四
## [中考改革第一枪](../../../教育/中考改革.md)
  9.97 11/26 HvS:/ h@o.Qx 五一刚刚结束，小学生的天就塌了！北京打响中考改革第一枪，90%的家长都浑然不知这背后的真相，其实最受影响的是小学# 育儿经验分享 # 小学教育 # 孩子教育 # 教育问题值得深思 # 家长必读  https://v.douyin.com/EusS8-ujmBM/ 复制此链接，打开Dou音搜索，直接观看视频！
  1. 科技强国
  2. 体育 & 道德
  3. 英语

  科技兴国 实干兴邦

  1. 语文 阅读理解
    清华附小 推荐 100本书
    教育部推荐书单 300字
    大语文国学
  2. 数学
    DK 图解数学
    超级课堂
    高斯数学
  3. 英语
    积累 词汇&语感，遵循 听说读写 规律
    英语动画

## 今日三件事
  1. 体检
  2. 护士资格证挂靠
  3. 机油

## 阅读文章
  [优秀孩子四个点](../../../教育/优秀孩子四个点.md)

# 2025年5月9日 周五

# 2025年5月15日 周四
## WEB3.0 讨论
  + Q: 我是一个web前端开发，掌握 html、js、css。typescript、python 等变成语言。
    我想从事web3开发，都需要掌握哪些技术？

  Gemini:
    以下是你 transitioning 到 Web3 开发需要掌握的关键技术和领域：

    **一、核心概念理解 (Conceptual Understanding):**
      在深入具体技术之前，强烈建议你先理解 `Web3 核心理念和基本原理`：

      + 区块链基础 (Blockchain Fundamentals):
        - 去中心化 (Decentralization): 理解其含义、优势和挑战。
        - 分布式账本技术 (Distributed Ledger Technology - DLT): 区块链是如何运作的。
        - 共识机制 (Consensus Mechanisms): 例如工作量证明 (Proof-of-Work), 权益证明 (Proof-of-Stake) 等。
        - 区块 (Blocks)、交易 (Transactions)、哈希 (Hashes): 区块链的基本构成单元。
        - 公钥/私钥 (Public/Private Keys) 和钱包 (Wallets): 用户如何与区块链交互和管理资产。
      + 智能合约 (Smart Contracts):
        - 定义: 自动执行、控制或记录法律相关事件和行为的计算机程序。
        - 应用场景: DeFi (去中心化金融), NFTs (非同质化代币), DAOs (去中心化自治组织) 等。
      + Web3 架构 (Web3 Architecture):
        - 与传统 Web2 应用的区别。
        - DApps (Decentralized Applications - 去中心化应用) 的构成。

    **二、智能合约开发 (Smart Contract Development):**
      这是 Web3 后端的核心，即使作为前端开发，了解其原理和交互方式也至关重要，甚至你可以考虑全栈发展。
      + Solidity:
        * 这是以太坊 (Ethereum) 以及其他兼容 EVM (Ethereum Virtual Machine) 的区块链（如 Polygon, BNB Chain, Avalanche C-Chain 等）上最主流的智能合约编程语言。你需要深入学习其语法、特性、设计模式和安全最佳实践。
        * 关键知识点:
          - 数据类型 (Data Types)
          - 函数 (Functions) 和修饰器 (Modifiers)
          - 事件 (Events)
          - 继承 (Inheritance)
          - 错误处理 (Error Handling: require, revert, assert)
          - Gas 优化 (Gas Optimization)
          - 安全漏洞防范 (e.g., Reentrancy, Integer Overflow/Underflow)

      + 开发框架 (Development Frameworks):
        * Hardhat: 一个非常流行的以太坊开发环境，提供了编译、部署、测试和调试智能合约的工具。它基于 JavaScript/TypeScript，对你来说会比较友好。
        * Truffle: 另一个成熟的开发框架，也提供了类似的编译、测试和部署功能。

      + 测试 (Testing):
        学习编写单元测试和集成测试来确保智能合约的正确性和安全性。框架如 Hardhat 和 Truffle 都内置了测试工具。
      + 调试 (Debugging):
        学习使用 Remix IDE 或 Hardhat 提供的工具进行智能合约调试。
    
    **三、与区块链交互 (Interacting with Blockchains - Frontend Focus):**
      这部分是你作为前端开发者将直接参与和构建的。
      
      + JavaScript 库 (JavaScript Libraries):
        * Ethers.js: 目前非常流行且功能强大的库，用于与以太坊区块链及其生态系统进行交互。它提供了钱包操作、合约交互、事件监听等丰富功能，并且有良好的 TypeScript 支持。
        * Web3.js: 历史悠久且广泛使用的库，功能与 Ethers.js 类似。
        * 你需要学习如何使用这些库来：
          - 连接到区块链网络 (例如通过 Infura, Alchemy 或本地节点)。
          - 读取区块链数据 (例如账户余额、合约状态)。
          - 发送交易 (例如调用智能合约的写方法)。
          - 监听智能合约事件。
      + 钱包集成 (Wallet Integration):
        * MetaMask: 最流行的浏览器钱包扩展。你需要学习如何让你的 DApp 与 MetaMask (或其他钱包如 WalletConnect, Coinbase Wallet) 进行连接，请求用户授权，获取账户信息，并代表用户发起交易。
        * 了解 EIP-1193 (Ethereum Provider JavaScript API)。
      + RPC 节点服务 (RPC Node Providers):
        * Infura, Alchemy, QuickNode: 这些服务提供了对区块链网络的访问节点，你无需自己搭建和维护全节点。了解它们如何工作以及如何在你的应用中使用。
      + 前端框架/库 (Frontend Frameworks/Libraries):
        * 继续使用你熟悉的 React, Vue, Angular 等。Web3 的交互逻辑可以很好地集成到现有的前端框架中。
        * 有一些 Web3 特定的 UI 组件库可以帮助你快速构建界面 (例如 web3-ui-kits)。
    
    **四、去中心化存储 (Decentralized Storage):**
      对于 DApp 的前端资源（HTML, CSS, JS, 图片等）以及用户生成的内容，通常会考虑使用去中心化存储方案。
      + IPFS (InterPlanetary File System): 一种点对点分布式文件系统。学习如何将文件上传到 IPFS 并通过其 CID (Content Identifier) 进行访问。
      + Arweave: 提供永久性去中心化数据存储。
      + Swarm: 另一个去中心化存储和通信平台。
    
    **五、后端和 API (Backend and APIs - 可选，但有益):**
      虽然你专注于前端，但了解一些后端概念和工具对构建复杂的 DApp 会很有帮助。你的 Python 和 JS (Node.js) 知识在这里会派上用场。
      + Node.js: 常用于构建 DApp 的后端服务、处理链下逻辑、缓存数据或作为与区块链交互的代理。
      + The Graph: 一个用于索引和查询区块链数据的去中心化协议。它可以让你高效地查询复杂的链上数据，而无需直接与 RPC 节点进行低效的交互。
      + 传统后端技术: 有时，DApps 仍然需要传统的后端来处理用户身份验证（链下部分）、数据缓存、发送通知等。

    **六、其他工具和技术 (Other Tools and Technologies):**
    **六、其他重要技能和工具 (Other Important Skills & Tools):**
      + 版本控制 (Version Control): Git (你应该已经掌握)。
      + 命令行 (Command Line): 熟练使用命令行对于开发和部署至关重要。
      + 开发工具 (Development Tools):
        * Remix IDE: 一个基于浏览器的 Solidity IDE，非常适合快速学习和原型开发。
        * VS Code + 插件: 例如 Solidity 插件，ESLint 等。
      + 安全性 (Security):
        * 智能合约安全审计 (Smart Contract Auditing): 了解常见的安全漏洞和审计流程。
        * 前端安全: 防范钓鱼攻击，保护用户私钥等。
      + 理解不同的区块链和 Layer 2 方案 (Understanding Different Blockchains & Layer 2s):
        * 除了以太坊，了解其他公链如 Solana, Polkadot, Cosmos 等的特点。
        * 了解 Layer 2 扩容方案如 Optimism, Arbitrum, zkSync, StarkNet，它们旨在提高交易速度并降低成本。
      + 学习资源和社区 (Learning Resources & Community):
        * 官方文档 (Ethereum.org, Solidity Docs, Ethers.js Docs 等)。
        * 在线课程 (CryptoZombies, Encode Club, Udemy, Coursera 等)。
        * 参与社区 (Twitter, Discord, Telegram 上的 Web3 项目和开发者社群)。
        * 阅读白皮书和技术博客。
    
    **学习路径建议:**
      + 巩固基础: 确保你对 JavaScript 和 TypeScript 的异步编程、Promise、模块化等有深入理解。
      + 理解 Web3 核心概念: 花时间学习区块链和智能合约的基本原理。
      + 学习 Solidity: 从基础语法开始，逐步深入到合约设计和安全。Remix IDE 是一个很好的起点。
      + 掌握 Hardhat/Truffle: 选择一个框架进行智能合约的编译、测试和部署。
      + 学习 Ethers.js/Web3.js: 练习如何通过前端与智能合约进行交互，集成钱包。
      + 构建小型项目: 尝试构建一些简单的 DApp，例如投票应用、简单的 NFT 项目或 DeFi 应用。
      + 学习 IPFS: 了解如何存储和检索去中心化数据。
      + 关注安全: 始终将安全放在首位。
      + 持续学习和实践: Web3 技术发展迅速，保持学习的热情和持续实践非常重要。

    你的 Python 知识在 Web3 领域也可能有用武之地，例如用于编写脚本、进行数据分析、或者在某些非 EVM 链的智能合约开发（如 Algorand 的 PyTeal）。

    祝你在 Web3 的学习和开发旅程中一切顺利！这是一个充满创新和机遇的领域，你的前端背景将是一个巨大的优势。
    对于具备 HTML、JS、CSS、TypeScript 和 Python 基础的 Web 前端开发者来说，进入 Web3 开发领域需要掌握一系列新的技术和概念。这不仅涉及到前端的交互逻辑，更要深入理解区块链的后端运作方式。

    以下是您需要重点掌握的技术：

    **一、 核心概念理解：**
      + 区块链基础： 首先要理解区块链的基本原理，包括去中心化、分布式账本、共识机制（如 PoW, PoS）、区块、交易、哈希等概念。
      + 智能合约 (Smart Contracts)： 这是 Web3 的核心，是部署在区块链上并自动执行协议条款的代码。您需要了解智能合约能做什么、如何运作以及其局限性。
      + 去中心化应用 (DApps)： 理解 DApp 与传统 Web 应用的区别，DApp 的架构通常包括前端用户界面、与区块链交互的逻辑以及智能合约。
      + 钱包 (Wallets)： 例如 MetaMask、Trust Wallet 等。了解钱包的类型（托管型、非托管型）、功能（管理私钥、签署交易、与 DApp 交互）以及如何在前端集成它们。
      + Gas 费： 理解在以太坊等区块链上执行交易和部署智能合约需要支付 Gas 费的概念，以及 Gas 价格和 Gas Limit 的含义。
      + Token 标准： 了解常见的 Token 标准，如 ERC-20（同质化代币）和 ERC-721（非同质化代币/NFT）。

    **二、 智能合约开发：**
      + Solidity： 这是以太坊以及其他兼容 EVM（以太坊虚拟机）的区块链（如 Polygon, BNB Chain）上最主流的智能合约编程语言。鉴于您已掌握多种编程语言，学习 Solidity 的语法曲线应该不会太陡峭。
      + Rust (可选，但重要性日益增加)： Solana、Polkadot 等新兴公链选择 Rust 作为智能合约开发语言。如果您希望在这些生态发展，Rust 将是重要技能。
      + 智能合约开发框架：
        * Hardhat： 一个以太坊开发环境，提供编译、部署、测试和调试智能合约的工具。
        * Truffle： 另一个流行的以太坊开发框架，提供类似的功能。
      + 智能合约安全： 学习常见的智能合约漏洞（如重入攻击、整数溢出等）和最佳安全实践至关重要。可以了解 OpenZeppelin 等提供的安全合约库。

    **三、 前端与区块链交互：**
      + JavaScript 库：
        * Ethers.js / Web3.js： 这两个是与以太坊区块链交互最核心的 JavaScript 库。它们允许您的前端应用读取区块链数据、监听事件、连接用户钱包并发送交易到智能合约。Ethers.js 因其现代化的 API 和更小的体积而越来越受欢迎。
        * Viem： 一个新兴的 TypeScript 优先的库，旨在提供比 Ethers.js 和 Web3.js 更轻量级、更模块化的选择。
      + 前端框架： 您现有的 React, Vue, Angular 等前端框架知识依然适用。Web3 的交互逻辑通常会整合到这些框架中。
      + 状态管理： 在 DApp 中有效管理区块链状态和用户状态非常重要。
      + Wallet 连接库：
        * Web3Modal / RainbowKit / ConnectKit / Wagmi： 这些库简化了在 DApp 中集成多种钱包（MetaMask, WalletConnect, Coinbase Wallet 等）的过程，提供了标准化的用户界面和连接逻辑。wagmi 是一个流行的 React Hooks 库，用于与以太坊交互，通常与上述连接器库结合使用。

    **四、 去中心化存储 (Decentralized Storage)：**
      + IPFS (InterPlanetary File System)： 一种点对点的分布式文件系统，常用于存储 DApp 的前端代码、NFT 的元数据和媒体文件。
      + Arweave： 提供永久性、去中心化数据存储的解决方案。
      + Filecoin： 一个基于 IPFS 的去中心化存储网络，通过激励机制鼓励用户提供存储空间。

    **五、 后端和基础设施 (理解层面)：**
      + 节点 (Nodes)： 理解区块链节点的作用，它们存储区块链数据并验证交易。虽然前端开发不直接运行节点，但了解其交互方式（如通过 Infura, Alchemy 等节点服务提供商）是必要的。
      + 预言机 (Oracles)： 例如 Chainlink。智能合约本身无法直接访问链下数据（如真实世界的天气、股票价格等）。预言机是连接区块链与外部世界数据源的桥梁。
      + The Graph Protocol： 一个用于索引和查询区块链数据的去中心化协议，可以极大地简化 DApp 从区块链获取数据的过程。
      + Layer 2 扩展方案： 如 Optimistic Rollups (Optimism, Arbitrum) 和 ZK Rollups (zkSync, StarkNet)。了解它们如何帮助提高交易吞吐量并降低 Gas 费用。

    **六、 开发工具和平台：**
      + Remix IDE： 一个基于浏览器的 Solidity IDE，方便快速编写、测试和部署智能合约。
      + Ganache： 用于在本地创建个人以太坊区块链，方便开发和测试，无需消耗真实的 ETH。
      + 区块浏览器 (Block Explorers)： 如 Etherscan (以太坊)、Solscan (Solana)。用于查看链上交易、合约代码、账户余额等信息，是调试和理解链上活动的重要工具。

    **七、Python 在 Web3 中的应用：**
      您已掌握的 Python 可以在 Web3 开发中发挥作用，例如：
      + 脚本编写： 用于自动化部署、与智能合约交互、数据分析等。
      + 后端开发： 如果 DApp 需要中心化后端处理某些逻辑（虽然目标是去中心化，但有时是必要的），Python 的 Web 框架（如 Django, Flask）可以胜任。
      + 数据分析和机器学习： 分析链上数据，构建预测模型等。
      + 测试： 一些测试框架支持 Python。


    **学习路径建议：**
      + 深入理解区块链和以太坊： 这是基础。
      + 学习 Solidity 和智能合约开发： 从基础语法开始，然后是 ERC-20, ERC-721 等标准合约的实现。利用 Remix 进行练习。
      + 掌握 Ethers.js (或 Viem/Web3.js)： 学习如何在前端连接钱包、读取链上数据、调用智能合约函数、发送交易。
      + 实践项目： 从简单的 DApp 开始，例如一个投票应用、一个简单的 NFT 项目或者一个基本的 DeFi 应用。
      + 学习前端集成： 将您的 Web3 逻辑与 React/Vue/Angular 等框架结合，使用 Wagmi 等工具简化开发。
      + 了解去中心化存储： 尝试使用 IPFS 存储前端或 NFT 元数据。
      + 关注行业动态和安全： Web3 发展迅速，新技术和安全漏洞层出不穷，保持学习非常重要。

    鉴于您已经有扎实的前端和编程基础，转型 Web3 并非从零开始。关键在于理解区块链的独特范式，并掌握与之交互的新工具和语言。祝您在 Web3 的探索之旅一切顺利！

# 2025年5月16日 周五
## [恋爱十大效应](../../../读书笔记/心理学/恋爱十大效应.md)
  1. 互补效应
  2. 曝光效应
  3. 吸引力法则
  4. 曝光效应
  5. 相似性吸引效应
  6. 社会认同效应
  7. 情感感染效应
  8. 亲密接触效应
  9. 嫉妒效应
  10. 爱情错觉效应

  + 沉没成本

# 2025年5月19日 周一
  93年，已婚已育。作为过来人，我的建议是 结婚随缘。孩子能不要就别要。
  我的观点如下
  1. 婚姻制度 崩溃消亡是迟早的事情，结婚证 保护的是财产，而不是爱情。
    原因有 女拳，彩礼，女性经济独立 对男人的依附程度 史上最低
    婚后 双方的生活质量 都 大打折扣。（内心OS，我个人认为，大部分青壮年男性，主要是性需求，和少部分心理需求。 生理需求可以通过 其他很多途径得到解决，这里不再展开。心理需求，其实大部分异性给不到。而且 不结婚 不代表不谈恋爱。 单身可以有更多接触异性的机会。 如果结婚了，还出去沾花惹草 被抓包会很麻烦。换位思考，自己也不希望被绿）

  2. 孩子 很可爱，很美好！但是能不要就别要！
    原因 严重消耗金钱和精力。
    1. 除非 双方父母帮忙带娃，否则 妻子就不能上班了。
    2. 孩子是一个超级吞金兽。受到 优生优育观念 影响，目前的养育环境就是 尽可能的给予孩子最好的，更好的。
      网络上各种制造焦虑。以前很多东西和概念都不存在，小孩子也能成长。
      但是现在，无缘无故多出来很多消费的概念。你不花这笔钱 你都觉得你不是个好父母。
    3. 如果妻子不上班，需要你自己上班能养活 一家人。
      [房贷,车贷]这两个就能刷掉一大波 靠自己不啃老的牛马们。
    4. 现在的病毒肆虐，孩子特别容易生病
      小孩子生病了，比大人看病更贵，还需要请假带孩子去医院。
      花钱还没法工作
    5. 教育
      教育一个是精力，再一个就是耐心。
      上了一天班，回家还得陪娃玩，写作业。照顾娃吃饭洗漱
    6. 过去生娃是家庭生产力。现在生娃是社会牛马。
      花钱养出来的娃，拉动了社会发展。唯独家庭不会有任何收益。
      代际重复的过程，就是一代人养大另一代人。
    我不否认 养育子女，也觉得孩子天真烂漫，和孩子在一起痛并快乐着。但是 当下的社会环境，养娃是真的很累。
    目前就一个娃，不敢再要二胎了。

# 2025年5月21日 周三
# 2025年5月22日 周四
  + 安装 ssh工具，登录VPN(已完成)
  + 研究 服务器操作系统(5天)
    ```sh
      [root@localhost ~]# cat /etc/os-release
      NAME="Kylin Linux Advanced Server"
      VERSION="V10 (Lance)"
      ID="kylin"
      VERSION_ID="V10"
      PRETTY_NAME="Kylin Linux Advanced Server V10 (Lance)"
      ANSI_COLOR="0;31"

      [root@localhost ~]# uname -a
      'Linux localhost.localdomain 4.19.90-52.22.v2207.ky10.aarch64 #1 SMP Tue Mar 14 11:52:45 CST 2023 aarch64 aarch64 aarch64 GNU/Linux'
    ```
    NAME: 操作系统名称为“Kylin Linux Advanced Server”。
    VERSION: 版本号为“V10 (Lance)”。
    ID: 系统标识符为“kylin”。
    VERSION_ID: 版本标识符为“V10”。
    PRETTY_NAME: 显示的完整版本名称为“Kylin Linux Advanced Server V10 (Lance)”。
    ANSI_COLOR: 终端颜色设置为“0;31”。

    + CPU
      ```sh
        [root@localhost ~]# lscpu
        架构：                           aarch64
        CPU 运行模式：                   64-bit
        字节序：                         Little Endian
        CPU:                             64
        在线 CPU 列表：                  0-63
        每个核的线程数：                 1
        每个座的核数：                   32
        座：                             2
        NUMA 节点：                      2
        厂商 ID：                        HiSilicon
        型号：                           0
        型号名称：                       Kunpeng-920
        步进：                           0x1
        CPU 最大 MHz：                   2600.0000
        CPU 最小 MHz：                   200.0000
        BogoMIPS：                       200.00
        L1d 缓存：                       4 MiB
        L1i 缓存：                       4 MiB
        L2 缓存：                        32 MiB
        L3 缓存：                        64 MiB
        NUMA 节点0 CPU：                 0-31
        NUMA 节点1 CPU：                 32-63
        Vulnerability Itlb multihit:     Not affected
        Vulnerability L1tf:              Not affected
        Vulnerability Mds:               Not affected
        Vulnerability Meltdown:          Not affected
        Vulnerability Mmio stale data:   Not affected
        Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl
        Vulnerability Spectre v1:        Mitigation; __user pointer sanitization
        Vulnerability Spectre v2:        Not affected
        Vulnerability Srbds:             Not affected
        Vulnerability Tsx async abort:   Not affected
        标记：                           fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop
                                          asimddp asimdfhm ssbs
      ```
    + npu
      ```sh
        [root@localhost ~]# npu-smi info
        +--------------------------------------------------------------------------------------------------------+
        | npu-smi 24.1.0.1                                 Version: 24.1.0.1                                     |
        +-------------------------------+-----------------+------------------------------------------------------+
        | NPU     Name                  | Health          | Power(W)     Temp(C)           Hugepages-Usage(page) |
        | Chip    Device                | Bus-Id          | AICore(%)    Memory-Usage(MB)                        |
        +===============================+=================+======================================================+
        | 1       310P3                 | OK              | NA           47                0     / 0             |
        | 0       0                     | 0000:01:00.0    | 0            1894 / 44215                            |
        +===============================+=================+======================================================+
        +-------------------------------+-----------------+------------------------------------------------------+
        | NPU     Chip                  | Process id      | Process name             | Process memory(MB)        |
        +===============================+=================+======================================================+
        | No running processes found in NPU 1                                                                    |
        +===============================+=================+======================================================+

        [root@localhost ~]# npu-smi info
        +--------------------------------------------------------------------------------------------------------+
        | npu-smi 24.1.0.1                                 Version: 24.1.0.1                                     |
        +-------------------------------+-----------------+------------------------------------------------------+
        | NPU     Name                  | Health          | Power(W)     Temp(C)           Hugepages-Usage(page) |
        | Chip    Device                | Bus-Id          | AICore(%)    Memory-Usage(MB)                        |
        +===============================+=================+======================================================+
        | 1       310P3                 | OK              | NA           48                0     / 0             |
        | 0       0                     | 0000:01:00.0    | 0            1894 / 44215                            |
        +===============================+=================+======================================================+
        +-------------------------------+-----------------+------------------------------------------------------+
        | NPU     Chip                  | Process id      | Process name             | Process memory(MB)        |
        +===============================+=================+======================================================+
        | No running processes found in NPU 1                                                                    |
        +===============================+=================+======================================================+
      ```

    + 部署 python 环境
      - 确认系统架构
  + 部署 ollama 模型(2天)
    ```sh
      [root@localhost ~]# ls
      公共  视频  文档  音乐  anaconda-ks.cfg  Ascend-hdk-310p-npu-driver_24.1.0.1_linux-aarch64.run  log
      模板  图片  下载  桌面  ascend_check     initial-setup-ks.cfg                                   ollama-linux-arm64.tgz
      [root@localhost ~]# sudo tar -C /usr -xzf ollama-linux-arm64.tgz
      [root@localhost ~]# ls
      公共  视频  文档  音乐  anaconda-ks.cfg  Ascend-hdk-310p-npu-driver_24.1.0.1_linux-aarch64.run  log
      模板  图片  下载  桌面  ascend_check     initial-setup-ks.cfg                                   ollama-linux-arm64.tgz
      [root@localhost ~]# ll
      总用量 1795288
      drwxr-xr-x 2 root root          6  4月 15 17:11 公共
      drwxr-xr-x 2 root root          6  4月 15 17:11 模板
      drwxr-xr-x 2 root root          6  4月 15 17:11 视频
      drwxr-xr-x 2 root root          6  4月 15 17:11 图片
      drwxr-xr-x 2 root root          6  4月 15 17:11 文档
      drwxr-xr-x 2 root root          6  4月 15 17:11 下载
      drwxr-xr-x 2 root root          6  4月 15 17:11 音乐
      drwxr-xr-x 2 root root          6  4月 15 17:11 桌面
      -rw------- 1 root root       3421  4月 15 17:06 anaconda-ks.cfg
      dr-x------ 2 root root         86  4月 15 17:32 ascend_check
      -rwxr-xr-x 1 root root  142529552  4月 15 17:25 Ascend-hdk-310p-npu-driver_24.1.0.1_linux-aarch64.run
      -rw-r--r-- 1 root root       4004  4月 15 17:10 initial-setup-ks.cfg
      drwxr-xr-x 3 root root         22  4月 15 17:29 log
      -rw-r--r-- 1 root root 1695832508  5月 21 15:15 ollama-linux-arm64.tgz
      [root@localhost ~]# ollama -v
      ollama: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.25' not found (required by ollama)
      [root@localhost ~]# ollama -h
      ollama: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.25' not found (required by ollama)
    ```

    ```sh
      [root@localhost ~]# sudo tar -C /usr -xvzf ollama-linux-arm64.tgz
      bin/ollama
      lib/ollama/cuda_v11/
      lib/ollama/cuda_v11/libcudart.so.11.0
      lib/ollama/cuda_v11/libcudart.so.11.3.109
      lib/ollama/cuda_v11/libcublasLt.so.11
      lib/ollama/cuda_v11/libcublas.so.11.5.1.109
      lib/ollama/cuda_v11/libcublas.so.11
      lib/ollama/cuda_v11/libggml-cuda.so
      lib/ollama/cuda_v11/libcublasLt.so.11.5.1.109
      lib/ollama/cuda_v12/
      lib/ollama/cuda_v12/libcublasLt.so.12
      lib/ollama/cuda_v12/libcudart.so.12
      lib/ollama/cuda_v12/libcudart.so.12.8.90
      lib/ollama/cuda_v12/libcublas.so.12
      lib/ollama/cuda_v12/libcublasLt.so.12.8.4.1
      lib/ollama/cuda_v12/libcublas.so.12.8.4.1
      lib/ollama/cuda_v12/libggml-cuda.so
      lib/ollama/libggml-base.so
      lib/ollama/libggml-cpu.so
    ```

    ```sh
      [root@localhost ~]# lspci | grep VGA
      06:00.0 VGA compatible controller: Huawei Technologies Co., Ltd. Hi171x Series [iBMC Intelligent Management system chip w/VGA support] (rev 01)
      [root@localhost ~]# sudo lshw -C display
        *-display                 
            description: VGA compatible controller
            product: Hi171x Series [iBMC Intelligent Management system chip w/VGA support]
            vendor: Huawei Technologies Co., Ltd.
            physical id: 0
            bus info: pci@0000:06:00.0
            version: 01
            width: 32 bits
            clock: 33MHz
            capabilities: pm msi pciexpress vga_controller bus_master cap_list
            configuration: driver=hibmc-drm latency=0
            resources: irq:77 memory:e4000000-e5ffffff memory:e8000000-e81fffff
    ```
    + 解决GCC版本
    ```sh
    [root@localhost ~]# strings /usr/lib64/libstdc++.so.6 | grep GLIBCXX
    GLIBCXX_3.4
    GLIBCXX_3.4.1
    GLIBCXX_3.4.2
    GLIBCXX_3.4.3
    GLIBCXX_3.4.4
    GLIBCXX_3.4.5
    GLIBCXX_3.4.6
    GLIBCXX_3.4.7
    GLIBCXX_3.4.8
    GLIBCXX_3.4.9
    GLIBCXX_3.4.10
    GLIBCXX_3.4.11
    GLIBCXX_3.4.12
    GLIBCXX_3.4.13
    GLIBCXX_3.4.14
    GLIBCXX_3.4.15
    GLIBCXX_3.4.16
    GLIBCXX_3.4.17
    GLIBCXX_3.4.18
    GLIBCXX_3.4.19
    GLIBCXX_3.4.20
    GLIBCXX_3.4.21
    GLIBCXX_3.4.22
    GLIBCXX_3.4.23
    GLIBCXX_3.4.24
    GLIBCXX_DEBUG_MESSAGE_LENGTH
    ```
  + 解决 GCC 运行时库版本问题
    - 下载必要的rpm包
      > yum install --downloadonly --downloaddir=/tmp/gcc-packages gcc-c++ libstdc++ libgcc
      注意：如果无法直接下载，可以从麒麟官方镜像站下载对应的rpm包：
      - libstdc++-8.3.0 或更高版本
      - gcc-c++-8.3.0 或更高版本
      - gcc-8.3.0 或更高版本
    -  下载地址（ARM 官网）
      页面地址（Linux 版本）：
      https://developer.arm.com/downloads/-/arm-gnu-toolchain-downloads

      直接下载链接：
      > https://developer.arm.com/-/media/Files/downloads/gnu/14.2.rel1/binrel/arm-gnu-toolchain-14.2.rel1-x86_64-aarch64-none-linux-gnu.tar.xz

      你需要的是这个版本，因为它的 host 是 Linux (x86_64)，target 是 aarch64-linux-gnu，能包含你所需的 libstdc++.so.6.0.32。
    - 解压
      > tar -xf arm-gnu-toolchain-14.2.rel1-x86_64-aarch64-none-linux-gnu.tar.xz
    - 查找 libstdc++.so.6.0.32：
      > find ./ -name "libstdc++.so.6*"
    - 打包
      > cd /tmp/arm-gcc/arm-gnu-toolchain-14.2.rel1-x86_64-aarch64-none-linux-gnu/aarch64-none-linux-gnu/lib64/
      > tar -czvf libstdcxx-14.2-aarch64.tar.gz libstdc++.so.6 libstdc++.so.6.0.33
    - 新建文件夹
      > sudo mkdir -p /usr/local/ollama/lib
    - 解压
      > sudo tar -xzvf libstdcxx-14.2-aarch64.tar.gz -C /usr/local/ollama/lib
      > sudo tar -xzvf /tmp/arm-gcc/arm-gnu-toolchain-14.2.rel1-x86_64-aarch64-none-linux-gnu/aarch64-none-linux-gnu/lib64/libstdcxx-14.2-aarch64.tar.gz -C /usr/local/ollama/lib
    - 确认或创建符号链接（如果打包时未保留）
      ```bash
      cd /usr/local/ollama/lib
      sudo ln -sf libstdc++.so.6.0.33 libstdc++.so.6
      ```
    - 确认 Ollama 安装目录
      - 📍 方法一：which 或 command -v
      ```bash
        which ollama
        # 或
        command -v ollama
      ```
    - 
    - 验证是否使用新库
      > LD_LIBRARY_PATH=/usr/local/ollama/lib ldd /usr/bin/ollama | grep libstdc++

  + STT/TTS 离线库(李诚浩≈≈2天)
    arm64
  + 软件移植(3天)
    代码抽离
    功能测试
  + 前端界面优化(只提供Demo)

# 2025年5月22日 周四
  天气: 中雨
  今天开车来上班

## 华为云服务器 配置价格表
  + ![华为云服务器 配置价格表](07db1c0c1120e1c7a60a4869ab03fd43.png)

  + 2卡:32u128g就限定了卡的数量是2了，
    配置：核心数：32u；内存：128GB，显存：48GB，
    价格：按需费用：16.917/小时；包月:8,120，包年：97440；不含磁盘和带宽
  + arm

## [回复评论](https://mp.weixin.qq.com/s/N5-VRs46-Iui3ZjUs8V5pQ)
  爱美之心人皆有之，男性对美丽异性 压抑不住的性冲动，也是刻在DNA里面的底层代码。

  我无法否认你的观点。也不能说你的观点有错。

  但是，你的这个观点，总给人一种 道貌岸然，唱高调的反人性感觉。
  甭管人家是动刀还是打针了，直男看了脑袋里有点想法，大方承认有什么了？
  非得是个美女都娶回家？

  伟光正的人设，给自己增加游戏难度？还是说你就属于是 0.0000000000000001%？
  总之，能勇敢表达自己的观点 还是值得肯定。

## 同步工作进度
1. 离线安装 ollama
  解决 gcc 运行时库版本问题
    装了新版本，但是报缺少 其他的配套运行时
    主要是 不知道还缺什么，
    能联网，用 yum 安装解决 更快
2. 联系 华为云 arm64 Atlas 昇腾npu服务器

# 2025年5月23日 周五
  1. 确认 Atlas 500 能不能安装 银河麒麟 v10
  2. 跑通 ollama
  3. stt tts 找口碑 + 国产平台


# 2025年5月24日 周六
  ## 交强险赔付
    情况描述
    我驾驶机动车 和 对方 横穿马路电动车 发生轻微剐蹭。
      对方只是软组织挫伤。没有骨折等严重伤害。
    提出一大堆赔偿要求，这个不需要我来应付。有保险公司来处理。
      医药费、误工费、营养费、精神损失费、修车费 等
    但是，对方不愿赔付我的修车费用，扯皮。
    我的修车费用，最低诉求是 300元。并且还有涉及第二年保险费上涨问题。

    1. 保险理赔 有没有时间限制？(对方不赔付 修车费，那就不签字，不给报销对方的赔偿要求!)
    没有
    2. 保险费 上涨幅度？大概多少？

    3. 保险公司处理时间 超出后会不会过期？后续还需要我处理么？(一般是 30天)

    对方说不信任我，要我先签字。随后又反悔，提出不会理赔我修车费用。
    属于是 小人行径。吃相难看。既要又要。拿了各种赔偿，还不想给 修车费。


    我都打听过了，我这属于 合理诉求。你这边不是没有责任！
    所以你理应赔付 修车费用。我这边帮你签字，你和保险公司进行索赔。

    如果你拒绝赔付，修车费用。我有权利 拒绝保险签字。
    和解不了，我们走法律程序。

## 今天任务 部署 ollama
  + 提示词：
    我有一台 边缘计算服务器。
    已知是 Ubuntu 操作系统，NVIDIA 显卡。
    现在想知道 cpu架构，显卡具体参数。

    请你给出 命令行命令，帮我分析配置信息

  0. 查看 Ubuntu 配置
    CPU、GPU信息
  1. x86 还是 arm64？
  2. Ubuntu 如何 安装 ollama


  + 干掉本地磁盘 ollama 模型缓存
  rm -rf ~/.ollama/models/* /mnt/ssd/ollama/models/
  mv ~/.ollama/models/* /mnt/ssd/ollama/models/

## 命令
  ```sh
  root@t-desktop:/home/t# lspci | grep -i nvidia
  0005:00:00.0 PCI bridge: NVIDIA Corporation Device 229a (rev a1)
  root@t-desktop:/home/t# 
  root@t-desktop:/home/t# nvidia-smi
  bash: nvidia-smi: command not found
  root@t-desktop:/home/t# 

  ```

  ```sh
  root@t-desktop:/home/t# head -n 1 /etc/nv_tegra_release
  # R35 (release), REVISION: 1.0, GCID: 31346300, BOARD: t186ref, EABI: aarch64, DATE: Thu Aug 25 18:41:45 UTC 2022

  ```

  + 解决 aarch64-jetson NVIDIA 嵌入式平台 缺少 cuda 运行时的问题
  https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=aarch64-jetson&Compilation=Cross&Distribution=Ubuntu&target_version=22.04&target_type=deb_network_cross



  sudo mkdir -p /data/usr-src
  sudo rsync -aXS /usr/src/ /data/usr-src/
  sudo mv /usr/src /usr/src.bak
  sudo ln -s /data/usr-src /usr/src

# 2025年5月26日 周一
  + ![边缘计算服务器运行ollama](image.png)

  + 当前系统状态总结
    | 项目             | 状态                     | 说明                                                        |
    | --------------   | ----------------------  | ---------------------------------------------------------   |
    | **平台**         | Jetson (嵌入式 ARM 平台) | 有 `tegrastats` 和 30537MB RAM，推测是 Jetson AGX Xavier 或 Orin |
    | **GPU 驱动**     | 正常工作中               | `tegrastats` 中 `GR3D_FREQ` 有波动，表示 GPU 正常运行           |
    | **CUDA 工具链**  | 安装正常                 | `nvcc --version` 显示 CUDA 11.4 可用                          |
    | **nvidia-smi**   | 不存在且无需担心          | Jetson 系列设备不支持 `nvidia-smi`，这是预期行为               |
    | **显存占用**     | 很低，空闲状态            | `GR3D_FREQ` 波动不大，表示当前没有高 GPU 负载任务              |

  + 🔍 Ollama 报错原因分析
  ```sh
  Error: timed out waiting for llama runner to start - progress 0.00
  ```

  + 切换到适配 Jetson 的 LLM 推理方式（推荐）
  ✅ 推荐的可选方案：
  | 推理框架                  | 优势                 | 是否支持 Jetson         |
  | ------------------------ | -------------        | ----------------       |
  | **llama.cpp**            | C++ 编译、低资源可运行 | ✅ 需源码编译          |
  | **FastChat + vLLM**      | 高性能                | ⚠️ ARM 架构不友好      |
  | **ONNX + TensorRT**      | Jetson 官方推荐       | ✅ 高效但需模型转换     |
  | **transformers + torch** | 灵活                 | ⚠️ 需模型裁剪 + QLoRA   |

  + 使用 Jetson 上的 llama.cpp 编译运行（推荐）
  ```sh
  mkdir /data/llama.cpp
  cd /data/llama.cpp

  mkdir build
  cd build

  cmake ..
  cmake --build . --config Release -j
  ```
  
  ```sh
  sudo apt install cmake g++ make
  git clone https://github.com/ggerganov/llama.cpp
  cd llama.cpp
  make -j

  # 运行转换好的模型
  ./main -m models/ggml-qwen2-14b.bin -p "你好"
  ```

  - ✅ Jetson / ARM64 上使用 CMake 编译 llama.cpp 的正确方式如下：

  1. 第一步：安装 CMake
  ```sh
  cmake --version
  ```

  2. 🏗️ 第二步：创建构建目录 & 编译
  ```sh
  cd /data/llama.cpp
  mkdir build
  cd build
  cmake ..
  cmake --build . --config Release -j
  ```

  3. 📦 第三步：下载 GGUF 格式模型（以 TinyLLaMA 为例）
  从 Hugging Face 下载 TinyLLaMA 的 GGUF 模型文件，例如：

  https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF

  建议下载如下模型（体积小，适合 Jetson）：
  ```sh
  wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf -O ./models/tinyllama.gguf
  ```
  ⚠️ 没有 models 文件夹就手动建一下：mkdir -p models

  4. 🚀 第四步：运行模型
  ```sh
  ./main -m ./models/tinyllama.gguf -p "你好"
  ```

  5. ✅ 总结指令汇总
    ```sh
    cd /data/llama.cpp
    mkdir build && cd build
    cmake ..
    cmake --build . --config Release -j
    mkdir -p models
    wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf -O ./models/tinyllama.gguf
    ./main -m ./models/tinyllama.gguf -p "你好"
    ```

  + ✅ 下一步：开始编译
    现在你可以继续执行以下命令来编译 llama.cpp：
    ```sh
    cmake --build . --config Release -j
    ```
    参数说明：
    --build . 表示构建当前目录下的工程；
    --config Release 指定构建配置为 Release（一般用于提升性能）；
    -j 启用多线程编译，默认根据 CPU 核心数自动并行。

  + 从本地支持VPN x86 wsl Ubuntu 下载 Hugging Face 
    🧠 选择适合的 GGUF 模型格式
      llama.cpp 支持 GGUF 格式的模型。以下是一些可供选择的 Qwen3-0.6B GGUF 模型版本：
      
      Qwen/Qwen3-0.6B-GGUF：官方提供的 GGUF 格式模型。 
      
      ggml-org/Qwen3-0.6B-GGUF：由 ggml-org 提供的 GGUF 模型，支持多种量化格式，包括 8-bit 和 16-bit。 
      
      second-state/Qwen3-0.6B-GGUF：提供多种量化版本，如 Q4_K_M（484MB）、Q5_K_S（544MB）等，适用于不同的硬件资源。 
      
      bartowski/Qwen_Qwen3-0.6B-GGUF：包含多种量化版本，如 Q6_K_L（700MB）、Q5_K_L（650MB）等，适合不同的性能需求。 

      建议根据您的硬件资源选择合适的模型版本。例如，Q4_K_M 提供了良好的性能与模型大小的平衡，适合大多数本地部署场景。
    ```sh
    wget https://huggingface.co/second-state/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q4_K_M.gguf -O model.gguf

    ```

  + 下载模型
    Hugging Face 下载支持 ggml 的 Qwen2.5 14B 格式模型（或量化后的版本，gguf）

    1. 下载一个模型（如 TinyLLaMA）
    ```sh
    mkdir -p /data/llama.cpp/models/tinyllama
    cd /data/llama.cpp/models/tinyllama

    wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-GGUF/resolve/main/tinyllama-1.1b-chat.q4_0.gguf -O model.gguf
    ```

  ## 保留 CPU llama.cpp 版本, 重新部署 支持GPU llama.cpp 版本
    ✅ 操作步骤如下：
    1. 复制 llama.cpp 项目目录
      假设你当前的 CPU-only 项目在 /data/llama.cpp：

      ```sh
      cd /data
      cp -r llama.cpp llama-cuda
      这会创建一份新的副本 llama-cuda/。
      ```
    
    2. 编译 GPU 版本(启用 cuBLAS)
      ```sh
      cd /data/llama-cuda
      make clean
      make LLAMA_CUBLAS=1
      ```
    
    3. 测试 GPU 支持是否生效
      使用 --n-gpu-layers 运行 GPU 模型，例如：

      ```sh
      ./build/bin/llama-cli -m ./models/qwen/model.gguf --n-gpu-layers 28
      ```

      其中：
      --n-gpu-layers 28 表示使用 GPU 加速全部 28 层。
      你也可以用小值测试，例如 --n-gpu-layers 4 看显存使用情况。

    4. 保留两个版本：
      路径               功能                  编译方式
      /data/llama.cpp    CPU-only 模型运行     make
      /data/llama-cuda   GPU/cuBLAS 模型运行   make LLAMA_CUBLAS=1

      你可以分别使用两个不同的路径来运行模型，互不干扰。

    + ✅ 清理旧构建内容的方法
      如果你之前在 /data/llama-cuda/ 里用过 make 或 cmake，请执行：
      ```sh
      cd /data/llama-cuda/
      rm -rf build/ CMakeCache.txt CMakeFiles/ Makefile cmake_install.cmake
      ```
      这样就会完全清理掉旧的构建产物。
      
    + 重新执行 GPU 构建流程(报错流程，被弃用)
      ```sh
      mkdir build
      cd build
      cmake .. -DLLAMA_CUBLAS=ON
      cmake --build . --config Release -j
      ```

    + ✅正确的 GPU（CUDA/cuBLAS）构建方法
      ```sh
      cd /data/llama-cuda/
      rm -rf build/

      mkdir build
      cd build
      cmake .. -DGGML_CUDA=ON
      cmake --build . --config Release -j
      ```

      + 你已经成功走到了 CUDA 编译的配置阶段，但现在被下面这个问题卡住了：
        ❌ 问题：CMake 版本太低
          ```sh
          CMake 3.18 or higher is required. You are running version 3.16.3
          ```
        ✅ 解决方法：手动升级 CMake 到 ≥3.18
          下面是升级步骤（推荐方式是从源码编译安装最新 CMake）：

          1. 一、卸载旧版本（可选，但建议）
            ```sh
            sudo apt remove --purge cmake
            ```
          2. 二、从源码安装新版 CMake（推荐）
              ```sh
              cd /tmp
              wget https://github.com/Kitware/CMake/releases/download/v3.27.9/cmake-3.27.9.tar.gz
              tar -zxvf cmake-3.27.9.tar.gz
              cd cmake-3.27.9
              ./bootstrap
              make -j$(nproc)
              sudo make install
              ```
              你也可以替换为最新版本的链接：[CMake Releases](https://github.com/Kitware/CMake/releases)

    + [wsl文件管理](../../../../_posts/OperatingSystem/UnixLike/WSL/wsl文件管理.md)

  + CPU 版本运行 Qwen3 模型
  ```sh
  cd /data/llama.cpp
  ./build/bin/llama-cli -m ./models/qwen/model.gguf
  ```

  + GPU 版本运行 Qwen3 模型
  ```sh
  cd /data/llama-cuda
  ./build/bin/llama-cli -m ./models/qwen/model.gguf --n-gpu-layers 12 --n-threads 8 --ctx-size 4096
  ```

# 2025年5月27日 周二
  + [NVIDIA Jetson 嵌入式平台 大模型GPU运行分析](./GPU运行日志分析.md)

  + 今日目标
    1. 部署 qwen3:7b 模型
    2. 分析 GPU 运行日志
    3. 获取 模型每秒处理的 tokens 数
  
  + Token/s 自动化测试脚本
    ```sh
      #!/bin/bash

      MODEL_PATH="../models/qwen/model.gguf"
      PROMPT_TOKENS=4       # 使用简短 prompt
      GENERATE_TOKENS=128   # 生成 token 数量
      THREADS=8             # Jetson Orin 的核心数
      GPU_LAYERS=99         # 启用全部层的 GPU 加速

      echo "Testing with model: $MODEL_PATH"
      echo "Generating $GENERATE_TOKENS tokens using $THREADS threads and $GPU_LAYERS GPU layers..."

      # 运行 llama-bench 并提取生成性能
      ./bin/llama-bench -m "$MODEL_PATH" -p "$PROMPT_TOKENS" -n "$GENERATE_TOKENS" -t "$THREADS" --n-gpu-layers "$GPU_LAYERS" \
        | tee llama_bench.log \
        | grep -E "performance:|generation time"

    ```

  + 获取显存数
  
  + 下载 qwen3:8b 模型
    ```sh
    wget https://huggingface.co/TheBloke/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q6_K.gguf
    ```

  + TTS 寻找合适的开源库
    [EmotiVoice易魔声](https://github.com/netease-youdao/EmotiVoice?tab=readme-ov-file)
    [EmotiVoice易魔声Gitee](https://gitee.com/zhuaer/EmotiVoice?skip_mobile=true)

    ```sh
      docker run -dp 127.0.0.1:8501:8501 syq163/emoti-voice:latest
    ```

  + [文章审美](https://mp.weixin.qq.com/s/iYlWij1gKvfuZMHMVGsztg)
    ![美女图白裙](image-2.png)
    ![美女图黑裙](image-1.png)
    锐评:
      此种类型女子。既有先天之姿，又有后天习得之体态、气质、神韵、品味。
      属 美玉 + 精心刻意雕琢。乃是 有心之人 苦心培养之
      可做晋阶之径，亦可作化人无骨之害人利器。
      福薄命苦之辈，断断不可驭之。
      无能之人得之，轻则命途受损，重则殒命。




  ```sh
  ```

  ```sh
  ```

  ```sh
  ```




  
