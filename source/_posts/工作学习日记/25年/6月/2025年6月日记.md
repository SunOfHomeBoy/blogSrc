---
title: 2025年6月日记
date: 2025年6月3日 13:18:25
update: 2025年6月26日 14:55:34
tags: 日记
categories: Diary
---

# 2025年6月3日 13:18:52 周二
  节后第一天

  + [python环境搭建](./python环境搭建.md)


  + [三步恢复肝脏[健康]](./三步恢复肝脏.md)
    1. 戒糖(饮料、水果、各种含糖零食)
    2. 轻断食(晚上不吃饭！不管自己多么痛苦)
    3. 适量运动(尽量骑自行车，周末爬山)

# 2025年6月6日 周五
  + Q: 我是一个程序员，从开发的角度来说。目前都有哪些最新的AI技术？

  + A: 从开发者的角度来看，目前最新的AI技术主要集中在以下几个领域：
    1. 大语言模型（LLM）
      - GPT系列、LLaMA、PaLM等
      - 支持长上下文理解、多轮对话
      - 代码生成和解释能力
      - 应用：智能助手、代码补全、文档生成
    2. 多模态AI
      - 文本、图像、音频、视频的联合理解
      - CLIP、Stable Diffusion等模型
      - 应用：图像生成、视频理解、跨模态搜索
    3. 生成式AI
      - 文本生成：ChatGPT、Bard
      - 图像生成：DALL-E、Midjourney
      - 视频生成：Runway、Pika
      - 应用：内容创作、设计辅助
    4. 强化学习
      - AlphaGo、AlphaZero等
      - 应用：游戏AI、机器人控制、资源优化
    5. 边缘AI
      - 在设备端运行的轻量级模型
      - TensorFlow Lite、Core ML
      - 应用：移动端AI、物联网设备
    6. AI开发工具
      - LangChain：构建LLM应用
      - Hugging Face：模型库和工具
      - PyTorch 2.0：动态图优化
      - JAX：高性能数值计算
    7. AI安全与伦理
      - 模型解释性
      - 数据隐私保护
      - 对抗性攻击防御
    8. AI基础设施
      - 分布式训练框架
      - 模型压缩与量化
      - 高性能推理引擎
    9. 领域专用AI
      - 生物医药：蛋白质结构预测
      - 金融：风险预测、交易策略
      - 教育：个性化学习
    10. AI与区块链结合
      - 去中心化AI
      - 数据市场
      - 模型验证

    - 对于开发者来说，建议关注：
      1. 掌握主流AI框架（PyTorch、TensorFlow）
      2. 学习Prompt Engineering
      3. 了解模型微调和部署
      4. 关注AI安全与伦理
      5. 探索AI在新领域的应用

    这些技术正在快速发展，为开发者提供了广阔的机会和挑战。

  + 解决 ollama wsl 访问问题
    ```powershell
      $env:OLLAMA_HOST = "http://0.0.0.0:11434"
      ollama serve
    ```

    ```wsl.Ubuntu.sh
    ip route | grep default
      default via 172.27.176.1 dev eth0 proto kernel

    curl http://172.27.176.1:11434/api/tags
    ```

  + 下周工作计划
    + STT & TTS 代理接口开发
    + 解决 浏览器 https 跨域，跨协议 问题
    + 服务的 增加 接口报错日志记录
    + 数字人播报接口业务逻辑开发
    + 数据 和 话术 给到大模型，大模型进行自然语言处理
    + 数字人问题回答 接口业务逻辑开发

# 2025年6月9日 周一
  ## Ubuntu 安装docker
  以下是 ​​Ubuntu 系统更换清华大学镜像源（含 Docker 仓库）的完整步骤​​，结合网络优化和常见问题修复：

  + ​​一、更换 Ubuntu 系统镜像源​​
    +  步骤 1：备份原有源文件​​
      > sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak

    + ​​步骤 2：配置清华源​​
      ```sh
      # 清空旧配置（适用于 Ubuntu 20.04，其他版本替换 focal 为对应代号）
      > sudo sed -i 's@http://.*archive.ubuntu.com@https://mirrors.tuna.tsinghua.edu.cn@g' /etc/apt/sources.list
      > sudo sed -i 's@http://.*security.ubuntu.com@https://mirrors.tuna.tsinghua.edu.cn@g' /etc/apt/sources.list

      # 或手动写入（推荐）
      echo -e "deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse\n\
      deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse\n\
      deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse\n\
      deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse\n\
      deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse\n\
      deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse\n\
      deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse\n\
      deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse" | sudo tee /etc/apt/sources.list
      ```

    + ​​步骤 3：更新软件列表​​
      > sudo apt update && sudo apt upgrade -y

  + 二、配置 Docker 清华镜像源​​
    + ​​步骤 1：备份原有 Docker 源​​
      > sudo mkdir -p /etc/apt/sources.list.d/docker.backup
      > sudo mv /etc/apt/sources.list.d/docker.list* /etc/apt/sources.list.d/docker.backup/
    + ​​步骤 2：添加清华 Docker 仓库​​
      ```sh
      # 导入 GPG 密钥（清华镜像站提供）
      > sudo mkdir -p /usr/share/keyrings
      > sudo curl -fsSL https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

      # 写入仓库配置
      echo \
        "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] \
        https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu \
        focal stable" | \
        sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
      ​​步骤 3：更新并安装 Docker​​
      sudo apt update
      sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
      ```

  + ​​三、验证配置​
    + 1. 检查镜像源生效​​
      ```sh
      grep -A 3 "docker" /etc/apt/sources.list.d/docker.list
      预期输出：
      deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu focal stable
      ```
    ​​+ 2. 测试 Docker 安装​​
      > sudo systemctl start docker
      > sudo docker run hello-world  # 输出 "Hello from Docker!" 表示成功

  ## docker 运行 emoti-voice:v2
    > docker run -d -p 6005:6005 emoti-voice:v2

    > http://192.168.0.160:6005/docs
    > http://127.0.0.1:6005/docs // 本地回环网络

  ## 语音播报事件测试

  ## 接口报错log日志测试
    TTS Error
    > 2025-06-09 18:24:18,776 - ERROR - [2025-06-09 18:24:18.775836] TTS Error - Server returned error: Internal server error. Please check the logs for details.

# 2025年6月10日 周二
  我准备在 Ubuntu 服务器上部署 ollama docker镜像。
  但是 Ubuntu上 VPN不稳定，拉取失败。
  所以我准备 在 win11 上的 docker desktop 容器中 拉取 ollama docker 镜像，并在 Ubuntu 服务器上部署。
  请你给出具体方案
  所有系统 都是 x86_64 架构

  首先，请你用中文交流。
  其次，我想知道 如何搜索 ollama 的 docker 镜像。我想在 win11 docker desktop 上的 wsl Ubuntu 20.04 上拉取，然后 部署到我的 Ubuntu 服务器上。
  都是x86_64 架构

  + Ollama Docker 镜像打包
    1. 找到官方镜像
      > docker pull ollama/ollama:latest

    2. 本地运行 ollama/ollama:latest 镜像
      > docker run -d --name ollama_container -p 11434:11434 ollama/ollama:latest

    3. 检查 ollama 服务是否正常运行
      ```sh
      > docker run -d --name ollama_container -p 11434:11434 ollama/ollama:latest
      a09fee1963dfcfae27a470a4e883084e8caf3354ab66ffcd0dc6ff21b5d6f999
      > ollama -h
      ollama: command not found
      > docker ps
      CONTAINER ID   IMAGE                  COMMAND               CREATED          STATUS          PORTS                                             NAMES
      a09fee1963df   ollama/ollama:latest   "/bin/ollama serve"   53 seconds ago   Up 53 seconds   0.0.0.0:11434->11434/tcp, [::]:11434->11434/tcp   ollama_container
      ```
    4. 通过`浏览器` | `命令行`与 Ollama 交互
      + 4.1 访问本地 API 接口
        `http://172.27.176.1:11434/`
        `http://127.0.0.1:11434/api/tags`
      + 4.2 使用命令行操作 Ollama
        ```bash
        > docker exec -it ollama_container sh
        # ollama pull qwen3:32b
        7MB/s
        ```
        **等待 拉取模型文件**
    5. ollama 本地Docker镜像 打包
      - 基础信息
      ```sh
        > docker run -d --name ollama_container -p 11434:11434 ollama/ollama:latest
          a09fee1963dfcfae27a470a4e883084e8caf3354ab66ffcd0dc6ff21b5d6f999

        > docker ps
        CONTAINER ID   IMAGE                  COMMAND               CREATED       STATUS       PORTS                                             NAMES
        a09fee1963df   ollama/ollama:latest   "/bin/ollama serve"   2 hours ago   Up 2 hours   0.0.0.0:11434->11434/tcp, [::]:11434->11434/tcp   ollama_container

        > docker exec -it ollama_container sh
        # ollama list
        NAME         ID              SIZE     MODIFIED
        qwen3:32b    030ee887880f    20 GB    21 minutes ago
      ```

      1. 将容器提交为新镜像
        > docker commit ollama_container ollama_qwen3_32b_container

      2. 导出镜像 为 tar 文件：
        如果您需要将这个镜像传输到其他服务器或环境，您可以导出为 tar 文件并在目标环境中加载：

        > docker save -o ollama_qwen3_32b_container.tar ollama_qwen3_32b_container

    6. 目标服务器 copy镜像：

    7. 目标服务器 加载镜像：
      > docker load -i ollama_qwen3_32b_container.tar

    8. 服务器本地运行 ollama_qwen3_32b_container 镜像
      > docker run -d --name ollama_container -p 11434:11434 ollama_qwen3_32b_container

  + 数字人Agent 打包部署测试

  + 安装 Nginx

  ## 问题
    本地无法运行 32b 模型，提示内存不足。

    qwen3:32b模型 智能程度 比 qwen2.5:14b 更高

    我已经部署了 ollama docker 镜像。我现在需要 开启 docker GPU 支持

    1. 开启 docker GPU 支持(已开启 GPU支持)
      ollama 是否使用 GPU资源？
      目前通过资源监控，发现 CPU占用率 超高，但是 GPU占用率 0%。

    2. STT&TTS 接口无法访问，如何排查问题？
      docker exec -it elegant_sanderson bash
      * docker 包里 没有安装 调试工具，服务器安装速度、重新配置  需要时间

    3. 替换 qwen2.5:14b 模型

    4. LangChain 不支持 思考大模型，如何解决？

    5. LangChain 会 多轮思考，所以 等待时间就变更长了。
      如果 绕开LangChain 直接调用 Ollama API 响应速度还可以。

  ## 优化
    1. 流式响应
    2. 音频队列
    3. 音频ws接口

  ## 服务器 运行命令
    **STT&TTS**
    > docker run -d --name STT_TTS_container --restart unless-stopped -p 6005:6005 emoti-voice:v2
    **Ollama**
    > docker run -d --name ollama_container_GPU --restart unless-stopped -p 11434:11434 ollama_qwen3_32b_container
    **Ollama GPU 模式**
    > docker run -d --name ollama_container_GPU --restart unless-stopped -p 11434:11434 --gpus all -e NVIDIA_VISIBLE_DEVICES=0 -e OLLAMA_DEVICE=cuda ollama_qwen3_32b_container
      // -v /opt/ai/ollama:/root/.ollama \  # 挂载模型存储目录 * 可选配置 不需要
    ```sh
      # 检查 STT&TTS 容器
      docker ps -a | grep emoti-voice

      docker exec -it STT_TTS_container bash
    ```
    **数字人Agent**

  ## 钉钉工作日志
    + 调试 服务器
      + 安装并调试 Todesk、SSH 链接工具
      + 配置远程 唤醒服务器功能
    + 安装 VPN
    + 安装 docker
      + 部署 emoti-voice:v2 STT & TTS Docker包
      + 测试功能
    + 安装 ollama
      + VPN 问题，导致 下载总是出错
    + 编写 STT & TTS 服务，请求代理接口，并增加 log日志记录
      + 根据 服务返回错误日志，增加 拦截规则

    + 配置 Ubuntu 镜像源
      > cat /etc/apt/sources.list
    + 配置 docker 镜像源


# 2025年6月11日 周三
  ## [增加同型号 P40GPU](./多GPU支持.md)

  ## P40运行32b大模型文章分析
  1. https://blog.csdn.net/hai4321/article/details/146106017
  2. https://community.modelscope.cn/6740480fcd8b2677c3e4cc31.html

  ## [大模型同步](./大模型同步.md) 大模型 复制 copy

  ## 重新打包 STT&TTS docker镜像
    1. 创建镜像
      docker commit naughty_thompson stt_tts_serve:v1
    2. 验证镜像
      docker images
    3. 导出镜像
      docker save -o stt_tts_serve.tar stt_tts_serve:v1

    + 加载镜像
      docker load -i stt_tts_serve.tar
      docker run -d --name STT_TTS_container --restart unless-stopped -p 6005:6005 stt_tts_serve:v1

  ## 今日进展
  1. 解决大模型 14b 模型同步
  2. 32b 速度慢，有 LangChain 拖累，需要多次 调用 API
  3. 重新打包 STT&TTS docker镜像
    通过分析日志，原 docker 需要 联网 下载其他包
    在本地加载，downloading 完后 重新打包，即可解决

# 2025年6月12日 周四
  + ws 交互逻辑开发
    - 根据 话术，生成对应文字解析
  +

# 2025年6月13日 周五
  + 导出 Agent 镜像
    1. 导出conda 环境
    > conda env export > environment.yml
    2. pip 导出
    > pip freeze > requirements.txt

  + 构建 数字人镜像
    > docker build -f ./DigitalMan.dockerfile -t digitalman .

  + 运行调试容器
    > docker run -d --name digitalman_container -p 9880:9880 digitalman

  + 查看日志
    > docker logs digitalman_container

  + 导出 tar包
    > docker save -o digitalman.tar digitalman_container:v1
    > docker save -o digitalman.tar digitalman

  + 服务器 加载 tar包
    > docker load -i digitalman.tar

  + 服务器运行容器
    > docker run -d --name digitalman_container --restart unless-stopped -p 9880:9880 digitalman

  ## 解决 服务器 网络代理问题
    ```sh
      # 停止Clash后台服务
      sudo systemctl stop clash
      # 禁用开机自启
      sudo systemctl disable clash

      # 临时清除当前会话代理(切换其他 命令行 或 浏览器 不生效)
      unset http_proxy https_proxy all_proxy HTTP_PROXY HTTPS_PROXY ALL_PROXY

      # 永久清除（检查所有配置文件）
      grep -r "proxy" ~/.bashrc ~/.profile ~/.bash_profile /etc/environment 2>/dev/null
      # 找到后注释或删除相关行
      # 永久清除（检查以下文件）
      grep -r "proxy" ~/.bashrc ~/.profile ~/.b
    ```

  ## 配置docker镜像 apt-get镜像源

  ## 重新生成 ssl证书
    ```bash
      > os@os-S2600WFT:/etc/nginx/ssl$ pwd
      /etc/nginx/ssl
      > os@os-S2600WFT:/etc/nginx/ssl$ nano req.conf
      [req]
      distinguished_name = req_distinguished_name
      x509_extensions = v3_req
      prompt = no

      [req_distinguished_name]
      C = CN
      ST = Beijing
      L = Beijing
      O = ZSGX
      OU = DevTeam
      CN = 192.168.0.26

      [v3_req]
      keyUsage = keyEncipherment, dataEncipherment
      extendedKeyUsage = serverAuth
      subjectAltName = @alt_names

      [alt_names]
      IP.1 = 192.168.0.26
      DNS.1 = localhost
    ```
    重要：
      CN = 192.168.0.26：这里的CN（Common Name）最好也写成你的IP。
      IP.1 = 192.168.0.26：这是最关键的一行，它在SAN字段中指定了这个证书对该IP地址有效。

    + 生成新私钥
    ```sh
    # 生成新私钥(key.pem)和新的证书(cert.pem)
    openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout key.pem -out cert.pem -config req.conf
    ```
  + 重启 Nginx
    > sudo systemctl restart nginx
  + 测试 Nginx 配置是否正确
    > sudo nginx -t

  + Nginx 运行状态
    > sudo systemctl status nginx

  * 重新生成的证书有问题，恢复旧证书

  ## 分析大模型回复接口返回null问题
    + 前端优化，增加提示词
      `const msg = res.result || '对不起，我没听清楚，请再说一遍'`
    + 提取运行日志
      ```sh
        # 查找容器ID
        CONTAINER_ID=$(docker ps -aq --filter name=<容器名>)
        > docker ps -a

        # 清空日志文件(无须重启)
        sudo truncate -s 0 /var/lib/docker/containers/$CONTAINER_ID/$CONTAINER_ID-json.log

        # 日志文件路径(删除log文件需重启容器)
        sudo rm /var/lib/docker/containers/$CONTAINER_ID/$CONTAINER_ID-json.log
      ```
  + python3 一行命令 http服务
    python -m http.server 8000 // --bind 127.0.0.1

# 2025年6月14日 周六
  + 服务器 docker 镜像包
    ```sh
      os@os-S2600WFT:~/download_pub$ docker images
      REPOSITORY                   TAG                       IMAGE ID       CREATED         SIZE
      digitalman                   latest                    7d5a2b0bdded   25 hours ago    1.75GB /** 第二版 没有回答问题 */
      <none>                       <none>                    9b3a8568be54   28 hours ago    1.28GB /** 估计是第一版 */
      stt_tts_serve                v1                        552b3992f9fa   2 days ago      13.4GB
      ollama_qwen3_32b_container   latest                    e7c50030d6a6   4 days ago      23.7GB
      emoti-voice                  v2                        5417b338b259   9 days ago      13.3GB
      nvidia/cuda                  11.8.0-base-ubuntu22.04   1e75b7decac0   19 months ago   239MB
    ```

  + 保存之前V2 镜像
    ```bash
    # 提交容器为镜像
    docker commit digitalman_container digitalman_v2

    # 导出镜像为 tar
    docker save -o digitalman_v2.tar digitalman_v2

    # 其他服务器 加载验证
    docker load -i digitalman_v2.tar
    docker run -d --name digitalman_container --restart unless-stopped -p 9880:9880 digitalman_v2
    ```

  + 部署 易魔声 docker 服务到服务器
    ```bash
      docker pull syq163/emoti-voice:latest

      docker load -i syq163_emoti_voice.tar

      docker run -dp 0.0.0.0:8501:8501 -p 0.0.0.0:8000:8000 syq163_emoti_voice
    ```

# 2025年6月16日 周一
  + 易魔声 容器信息
    ```bash
      os@os-S2600WFT:~/SSh_Docker_DEV/devcontainer$ docker ps
      CONTAINER ID   IMAGE                        COMMAND                   CREATED        STATUS          PORTS                                                   NAMES
      6f75266c3ebf   syq163_emoti_voice           "/bin/sh -c 'streaml…"   40 hours ago   Up 16 minutes   0.0.0.0:8000->8000/tcp, 0.0.0.0:8501->8501/tcp          recursing_diffie
      4ad9a0b3b9e8   digitalman                   "python ./main.py"        42 hours ago   Up 20 minutes   0.0.0.0:9880->9880/tcp, [::]:9880->9880/tcp             digitalman_container
      fbbe6318a1e3   stt_tts_serve:v1             "python testV2.py"        4 days ago     Up 20 minutes   0.0.0.0:6005->6005/tcp, [::]:6005->6005/tcp, 8000/tcp   STT_TTS_container
      dcf9a775ca0e   ollama_qwen3_32b_container   "/bin/ollama serve"       5 days ago     Up 20 minutes   0.0.0.0:11434->11434/tcp, [::]:11434->11434/tcp         ollama_container_GPU
    ```

    ```sh
      # 使用docker inspect命令获取容器的完整配置信息，包括启动命令：
      > docker inspect --format='{{json .Config.Cmd}}' 6f75266c3ebf
      ["/bin/sh","-c","streamlit run demo_page.py --server.port 8501 & uvicorn openaiapi:app --reload --host 0.0.0.0 --port 8000"]
    ```
    从获取的信息来看，容器启动时执行了两个服务：

    Streamlit服务运行demo_page.py在8501端口

    Uvicorn服务运行openaiapi:app在8000端口

  + 易魔声 声音ID list
    ```sh
      os@os-S2600WFT:~/SSh_Docker_DEV/devcontainer$ docker exec recursing_diffie find /EmotiVoice -name "config.py"
      /EmotiVoice/config/joint/config.py
    ```

  + TTS 音频正则
    1. 获取容器命令
      ```sh
      > docker inspect --format='{{json .Config.Cmd}}' fbbe6318a1e3
      ```

  + 服务器 扩展安全维护(ESM)
    ```sh
      Welcome to Ubuntu 20.04.6 LTS (GNU/Linux 5.15.0-139-generic x86_64)

      * Documentation:  https://help.ubuntu.com
      * Management:     https://landscape.canonical.com
      * Support:        https://ubuntu.com/pro

      扩展安全维护（ESM）Infrastructure 未启用。

      4 更新可以立即应用。
      这些更新中有 4 个是标准安全更新。
      要查看这些附加更新，请运行：apt list --upgradable

      8 个额外的安全更新可以通过 ESM Infra 来获取安装。
      可通过以下途径了解如何启用 ESM Infra：for Ubuntu 20.04 at
      https://ubuntu.com/20-04

      New release '22.04.5 LTS' available.
      Run 'do-release-upgrade' to upgrade to it.

      Your Hardware Enablement Stack (HWE) is supported until April 2025.
      Last login: Mon Jun 16 09:48:44 2025 from 192.168.0.43
      Last login: Mon Jun 16 13:45:57 2025 from 192.168.0.43
    ```

  + TTS 进入报错崩溃 docker 容器的方法
    ```sh
      # 1. 停止当前容器
      docker stop STT_TTS_container

      # 2. 修改 docker-compose.yml，添加或修改命令以防止容器立即执行 testV2.py
      # 在 docker-compose.yml 中将 command 改为:
      command: tail -f /dev/null   # 这会让容器保持运行而不是直接执行 Python 脚本

      # 1. 启动容器
      docker-compose up -d

      # 2. 进入容器
      docker exec -it STT_TTS_container /bin/bash

      # 3. 在容器内安装 NLTK 资源
      python3 -c "import nltk; nltk.download('averaged_perceptron_tagger_eng')"

      # 4. 修复 testV2.py 的缩进问题
      vim /path/to/testV2.py

      # 用不同的启动命令覆盖容器的默认命令
      docker run -it --name STT_TTS_container_new image_name /bin/bash
    ```

    - 重新运行 STT_TTS_container 服务
    ```bash
      docker run -d --name STT_TTS_container_v2 --restart unless-stopped -p 6005:6005 stt_tts_serve:v1
    ```

    - 从容器中复制文件到主机
    ```bash
      # 从容器复制文件到主机
      docker cp STT_TTS_container:/app/testV2.py ./testV2.py
      # copy 新v2 文件到主机
      docker cp STT_TTS_container_v2:/app/testV2.py ./testV2_new.py

      # 修改文件后，再复制回容器
      docker cp ./testV2.py STT_TTS_container:/app/testV2.py
    ```

  + 检查 STT 音频服务docker中 是否存在 nltk 资源
    ```bash
    python -c "import nltk; print(nltk.__version__)"
      3.9.1

    python -c "import nltk; print(nltk.__version__)"
    ```

  + 公司购买VPN
    廉总: 公司统一购买了付费梯子，各位按需使用
      订阅连接：https://117.18.7.44:50001/api/v1/client/subscribe?token=e1cc594577d3e5975739f3965305ecd0
      Clash客户端软件下载（如果已经下载不用重复下载安装）：https://clashforwindows.net/files/Clash.for.Windows.Setup.0.20.39-Chinese.exe

# 2025年6月17日 周二
  + 问题记录
    1. 交通态势统计 接口
        当前的交通态势如下：
      - 在某个方向，120公里201米处出现了事件类型为1的情况，导致47米范围内的拥堵，平均车速为94km/h，能见度为32米。
      - 同样在另一个方向上，在120公里201米的位置发生了事件类型2的状况，造成约94米的交通堵塞，平均车辆行驶速度是93km/h，能见度下降到只有16米。

      * 事件类型逻辑修复
      * 方向逻辑修复

    2. 交通事件统计
      上周交通事件统计情况如下：
      统计时段: 2025-06-10 00:00:00 至 2025-06-17 23:59:59
      在统计时段内，发生了以下类型的交通事件：
      - 逆行事件：发生于proident eu culpa et方向4725568684837104公里-1030080290721908.5米处，在报告时间为2001年6月8日19点41分57秒。具体事件详情可能需要进一步查看相关记录以获取更详细信息。

      * 公里数字段
      * 方向字段

  + 封装 STT_TTS 正则模块
    ```sh
      # STT_TTS Docker 容器中 新增 字符过滤解析模块 修改文件后，再复制回容器
      docker cp ./parse_str.py STT_TTS_container:/app/parse_str.py

      # 替换 张嘴视频 修改文件后，再复制回容器
      docker cp ./open_mouth.mp4 digitalman_container:/app/public/video/open_mouth.mp4

      # 替换 闭嘴视频 修改文件后，再复制回容器
      docker cp ./shut_up.mp4 digitalman_container:/app/public/video/shut_up.mp4

      # 前端 STT_TTS 模块
      docker cp ./voiceSTT.js digitalman_container:/app/public/js/voiceSTT.js

      # 替换 智能体回答问题模块 修改文件后，再复制回容器
      docker cp ./digital_human_answers_questions_app.py digitalman_container:/app/src/LLMAgent/FunctionCalling/digital_human_answers_questions_app.py
    ```

  + 音频测试
    ```console
      AIChat().localTTS('目前道路交通状况如下：G15高速某某路段下行方向在31分钟前发生了一起逆行事件，该事件已经得到处理。上行方向车流量为55辆，平均车速达到113km/h；而下行方向则是45辆车，平均速度为79km/h。重点监控的两客一危车辆共24辆，在途过程中均未发现异常情况。以上是当前道路交通运行状态的简要介绍。')

      AIChat().localTTS('重点车辆 冀AFD888F 行驶速度: 200km/h, 目前行驶至 K20+300米处, 超速行驶 66.67%.')

      # 空白音频
      AIChat().localTTS('')
    ```

  + 微信汇报工作
    ```txt
      领导，跟您同步下进度，今天主要完成有
        1. 开发+测试 TTS正则批处理规则
        + 调整原来有冲突的规则
        + 新封装 正则处理模块
        + 测试过程中发现 并补足 语法规则
        + 正则替换多音字
            + 重点 -> 众点
            + 113km/h -> 13公里每小时
            +  K20+300米处，前端会处理字符串拼接
            + 接口返回文案优化，增加 换行符，增加 助动词
        2. 数字人形象调整，协助梦梦 进行格式替换
        + 更新了 张嘴说话的 数字人形象
        + MP4 不支持 透明度，改为 MOV 格式


        主要就是把 正则的规则 重新封装了一下，方便后期维护，添加新规则
        语音播报 还存在一些不完美的地方，今天测试的时候发现了不少 语音播报的 瑕疵点，又顺便补充了若干规则。
        只能随时发现随时增加 正则过滤规则了。

        另外，我准备明天找找有没有类似的 语音播报正则开源库，看看能不能用到咱们项目里面。
    ```

# 2025年6月18日 周三
  + VPN
    9点40 已使用 2.6GB

  +

# 2025年6月19日 周四
  ## 提示语
  + 以下是我为你生成的20条 正在查询提示语信息：
    1. 正在查询，请稍后。
    2. 正在思考，请稍等片刻。
    3. 请稍等，我正在为您查询相关信息。
    4. 稍等一下，我正在努力为您找到答案。
    5. 我正在搜索最佳答案，请耐心等待。
    6. 请稍候，我正在为您处理请求。
    7. 正在加载，请稍等。
    8. 正在努力为您找到答案，请稍候。
    9. 我正在查询，请您稍作等待。
    10. 请稍等，我会尽快为您提供帮助。
    11. 我正在为您查询，请稍等片刻。
    12. 请稍候，我正在为您寻找最佳解决方案。
    13. 正在为您查询，请耐心等待。
    14. 稍等一下，我正在为您处理您的请求。
    15. 我正在努力为您找到答案，请稍等。
    16. 请稍等，我正在为您查询相关信息。
    17. 正在为您查询，请稍候。
    18. 请稍候，我正在为您处理您的问题。
    19. 我正在为您查找答案，请耐心等待。
    20. 正在为您查询，请您稍作等待。

  + 优化方案
    0. 音频队列 优化(已完成)
    2. 增加 正在查询 语音提示 [提示语](#提示语)(已完成)
    1. 人物画面 底部增加 思考过程展示(UI 交互)

    3. 文案优化(领导您在给补充一下 具体细节)
    4. 视频形象 背景 优化 @王梦梦

    5. 【配置页】形象 声音 选择
    6. 【配置页】字幕显示隐藏
    7. 【配置页】字幕支持自定义位置，拖拽？

    [5,6,7放最后做]

  + docker 容器 添加 图片
    ```sj
      # 添加 图片icon
      docker cp ./star.png digitalman_container:/app/public/img/star.png
    ```

# 2025年6月20日 周五 请假
  梦到一个男人从 马路对面百米高楼一跃而下，摔到 马路对面。。。

# 2025年6月21日 周六
  + 前端提供模块化 JS脚本，供二次开发
  + 视频交互优化，根据 梦梦提供的视频，优化人物动画效果。
  + LangChain Agent 增加详细冗余的 日志输出，供前端展示 思考逻辑链路过程

# 2025年6月23日 周一
  ## 任务列表
    + [完成]配置 docker 服务地址 支持 本地回环网络
      网络通信原理 和 Docker网络模式
        - 127.0.0.1 或 localhost 是本地回环地址，用于访问本机服务。
        - 在容器中使用 127.0.0.1 指向 容器本身 本地地址，而不是 宿主机（服务器）本地地址。
      因此，在容器内访问 127.0.0.1，它不会转发到宿主机的服务上。

      ```sh
        # Docker 宿主机地址	172.17.0.1	容器可通过此 IP 访问宿主机服务
        > ip route
          default via 192.168.0.1 dev eno1 proto dhcp metric 100
          169.254.0.0/16 dev eno1 scope link metric 1000
          172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1
          192.168.0.0/24 dev eno1 proto kernel scope link src 192.168.0.26 metric 100
      ```

    + [完成]LangChain Agent 增加详细冗余的 日志输出，供前端展示 思考逻辑链路过程
      - 解决 UUID 和
      - 将 action 转换为可序列化的 dict

    + 上周、上月的 时间定义逻辑目前还不准确，需要优化

    + 增加 大模型心跳激活 逻辑
    + 把所有 docker 容器，再重新制作 新镜像，并本地保存
    + [AI数字人 前端产品化]  使用 Babylon 三维引擎，通过 canvas 优化数字人形象
        + 感觉这个可以放后期
        + 属于重要不紧急

  + 备份服务器 voiceSTT.js 文件

  + 更新容器 star.png icon文件

  + [升级 WebSocket 连接] 调整Nginx /nginx.conf 配置文件
    > pwd
    cat /etc/nginx/nginx.conf

    更新 配置文件，并在 服务器 和 本地更新 conf 配置文件

    ```sh
      > sudo nginx -t
        nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
        nginx: configuration file /etc/nginx/nginx.conf test is successful

      > sudo nginx -s reload

      # 查看 Nginx 报错日志
      > tail -f /var/log/nginx/error.log
      # 查看 后端日志 是否有连接到达
      tail -f digital_human_broadcast.log
    ```

  ## 内网穿透 Frp服务配置(接口联调业务)
    + [内网穿透] 配置 Ngrok
    + 192.168.0.250 账号密码：Administrator hbyh@2020
      服务地址: `E:\local\service`
      [Frp服务](./frp_0.51.0_windows_amd64-2.zip)

  ## 接口联调
    ```sh
    curl -v --location -X POST 'http://101.200.180.147:30000/api/hSpeed/api/roadTrafficStatus' \
      -H 'X-AppId: HLFY9T2d1z7MySY8hwGwh4' \
      -H 'X-Secret: pEyQm156SJ9iS7PbyjLCZ6' \
      -H 'User-Agent: Apifox/1.0.0 (https://apifox.com)' \
      -H 'Content-Type: application/json' \
      -H 'Accept: */*' \
      -H 'Host: 101.200.180.147:30000' \
      -H 'Connection: keep-alive' \
      -d ''
    ```

# 2025年6月24日 周二
  1. 更新标题 `AI数字人引擎`
  2. 视频优化 闪烁，背景色块
  3. 数字人 AB视频 display 切换
  3. ws 思考处理 链路数据

# 2025年6月25日 周三
  领导，文字同步下进度
    1. 数字人 视频优化
      + 增加 手部动作
    2. ws 思考处理 链路数据
      + 封装 思考链流式响应数据处理类
      + 优化前端交互逻辑
      + 增加换行逻辑
    3. 前端JS类模块封装
    4. 与平台方接口联调
      + 优化接口逻辑
      + 统一增加 timeWindow 入参字段，默认 60分钟
      + 增加 log日志
      + 记录接口入参、出参值，便于后续分析
      + 更新接口文档
      + 记录接口修改内容
  前面都是 优化工作。接口全部接通，但是 部分接口返回空数据，部分接口是 mock数据。数据质量，还不如我本地生成，跟我最开始生成的差不多，都是无意义数据。

# 2025年6月26日 周四
  1. 声音形象切换
    两男两女，闭嘴1 说话5(王梦梦实现中)
    前端初步实现, 还没有测试
      后期通过接口实现, 或 通过命名逻辑来实现
      需要告诉 前端，数字人形象代码

  2. [TTS音频优化](./TTS音频优化.md)
    2025年 这种分开读
    多音字、语气语调 优化
    `当前的交通运行状态整体平稳。在京港澳高速某路段上，没有发现重点车辆(两客一危)异常情况，在途的重点车辆也都处于正常行驶中。未报告任何突发或异常事件影响交通。请您在出行时关注实时路况信息，确保行车安全。`

  3. 声音形象切换 管理界面(往后放)
    前后端分离，提供纯JS逻辑类 即可。用户优化界面、交互

  * 备份 Docker 镜像! 勤备份!
    ```sh
      > docker ps -a
        CONTAINER ID   IMAGE                        COMMAND                   CREATED       STATUS                    PORTS                                                   NAMES
        0afd3cf00d18   stt_tts_serve:v1             "python testV2.py"        9 days ago    Exited (0) 9 days ago                                                             STT_TTS_container_v2
        6f75266c3ebf   syq163_emoti_voice           "/bin/sh -c 'streaml…"    11 days ago   Exited (137) 9 days ago                                                          recursing_diffie
        4ad9a0b3b9e8   digitalman                   "python ./main.py"        11 days ago   Up 28 minutes             0.0.0.0:9880->9880/tcp, [::]:9880->9880/tcp             digitalman_container
        fbbe6318a1e3   stt_tts_serve:v1             "python testV2.py"        2 weeks ago   Up 28 minutes             0.0.0.0:6005->6005/tcp, [::]:6005->6005/tcp, 8000/tcp   STT_TTS_container
        dcf9a775ca0e   ollama_qwen3_32b_container   "/bin/ollama serve"       2 weeks ago   Up 28 minutes             0.0.0.0:11434->11434/tcp, [::]:11434->11434/tcp         ollama_container_GPU

      > docker images
        REPOSITORY                   TAG                       IMAGE ID       CREATED         SIZE
        syq163_emoti_voice           latest                    8b8466a48b2f   11 days ago     8.37GB
        # digitalman_v2                latest                    b505258a7e01   11 days ago     1.78GB
        # digitalman                   latest                    98e11966c2da   11 days ago     2.22GB
        # <none>                       <none>                    9b3a8568be54   13 days ago     1.28GB
        # stt_tts_serve                v1                        552b3992f9fa   2 weeks ago     13.4GB
        # ollama_qwen3_32b_container   latest                    e7c50030d6a6   2 weeks ago     23.7GB
        emoti-voice                  v2                        5417b338b259   3 weeks ago     13.3GB
        nvidia/cuda                  11.8.0-base-ubuntu22.04   1e75b7decac0   19 months ago   239MB

      # 1. 将容器提交为新镜像
        > docker commit 容器名称|容器ID 新镜像名称:版本(版本非必须)

      # 2. 验证镜像 查看是否存在 `新镜像名称`
        > docker images

      # 3. 导出镜像 为 tar 文件：
        # 如果您需要将这个镜像传输到其他服务器或环境，您可以导出为 tar 文件并在目标环境中加载：
        > docker save -o 镜像tar包名称.tar 新镜像名称:版本(版本非必须)

      # 其他机器 加载镜像
        > docker load -i 镜像tar包名称.tar
        > docker run \
          -d \ # 后台运行容器
          --name 自定义容器名称 \ # 自定义容器名称
          --restart unless-stopped \ # 容器退出时自动重启
          -p 6005:6005 \ # 端口映射
          新镜像名称:版本(版本非必须)

      # 导出 STT_TTS_Server 镜像(镜像名称 必须小写!)
      > docker commit STT_TTS_container stt_tts_server:V1.0

      # 导出 Digitalman 镜像
      > docker commit digitalman_container digitalman:V1.0

      # 导出 Ollama_Qwen3_32b 镜像
      > docker commit ollama_container_GPU ollama_qwen3_32b_gpu:V1.0
    ```

  5. 领导指派任务 根据提供的 音频ID 生成对话 TTS，每个ID 生成两份
    语气: 庄重
    男声: [2067, 2229, 70, 753]
    女声: [3757, 103, 667, 8197]
    男声默认 2067
    女声默认 3757

    + 通过 服务器 generate_audios/generate_tts.py 脚本批量生成

    + 压缩音频文件夹
      zip -r audios.zip audios/
    + python3 一行命令 启动 HTTP静态资源服务端
      python3 -m http.server 8080

  6. 提示词告诉 大模型 作为 `交通管理者` 回答问题

  7. 时间幻觉
    提示词 告诉大模型 每次回答问题 之前，先 查询系统日期

  8. 年份 和 时分秒 得单独处理一下

  9. 确认 接口是否 500
    /api/keyVehicleStatus: 重点车辆接口 返回值 格式不正确
    ```json
    {
        "code": 200,
        "msg": "Success",
        "data": {
            "data": []
        }
    }
    ```

    /api/deviceStatus: 设备运行情况接口 返回值 类型 不正确
    ```json
    {
        "code": 200,
        "msg": "Success",
        "data": {
            "data": [
              {
                "deviceType": "路面状况检测仪",
                "deviceName": "未知设备",
                "deviceTotal": 2,
                "abnormalList": []
              },
              {
                "deviceType": "路侧智能感知装置",
                "deviceName": "未知设备",
                "deviceTotal": 41,
                "abnormalList": []
              },
            ]
        }
    }
    ```


  ## 工作汇报
    1. 讲话动作切换
      已实现前端代码 待测试
      等待视频提供

    2. TTS音频优化
        专业TTS系统都会有
        + 前端文本标准化（Text Normalization, TN）
        + 文本清洗（Text Preprocessing） 模块
        步骤是把`“原始文本”转换为“适合语音合成”`的标准文本

        方案1 继续往现有正则模块 手动增加规则
        方案2 使用 `自然语言处理（NLP）工具库`

    3. 优化大模型
        优化 角色提示词
          提示词告诉 大模型 作为 `交通管理者` 回答问题
        优化 时间幻觉，强制查询 系统日期

    4. 测试接口返回值格式
      向对方开发反馈接口问题，沟通增加 真实测试数据
      /api/keyVehicleStatus: 重点车辆接口 返回值 格式不正确

# 2025年6月27日 周五
  + 更新 Qwen3 14b 模型 替换 Qwen2.5 14b 模型

  + Ollama 如何关闭 Qwen3 14b思考模式？
    1. 我使用 ollama docker镜像 启动运行了 Qwen3:14b 容器
    2. 同时我制作了 使用 LangChain 的 python Agent 程序

    使用 langchain_ollama  如何关闭 Qwen3 14b思考模式？
    ```python
      from langchain_ollama import OllamaLLM
      # 初始化本地模型
      llm = OllamaLLM(base_url=base_url, model='qwen3:14b')

      # 禁用思考模式
      llm = OllamaLLM(
          base_url=base_url,
          model="qwen3:14b",
          client_kwargs={"think": False} # chatGpt 建议我 增加这一行配置代码
      )
      # 但是 GPT 说，ollama 版本要 0.6.x 以上版本才能支持 关闭思考模式
    ```

    ```sh
    pip show ollama
    pip list | grep ollama
    ```

    ```sh
      > ollama show --modelfile qwen3:14b > modelfile.txt

      # 更新 modelfile.txt 文件 到 容器中
      docker cp modelfile.txt ollama_container_GPU:/mnt/modelfile.txt

      # 保存修改后的 Modelfile，通过 ollama build 命令重新生成模型
      > ollama build qwen3_nothink:14b < /mnt/modified/modelfile.txt

      # 需要备份 modelfile.txt!
    ```

  + 查看 Ollama 服务端日志：
    ```sh
      > docker logs ollama_container_GPU | grep "Thinking mode"
    ```

  + 查找 Ollama 模型配置文件路径
    ```sh
      > /root/.ollama/models/qwen3/14b/Modelfile

      # 分析输出
      root@dcf9a775ca0e:~/.ollama/models/manifests/registry.ollama.ai/library/qwen3# ls
      14b  32b

      root@dcf9a775ca0e:~/.ollama/models/manifests/registry.ollama.ai/library/qwen3# pwd
      /root/.ollama/models/manifests/registry.ollama.ai/library/qwen3

      root@dcf9a775ca0e:~/.ollama/models/manifests/registry.ollama.ai/library/qwen3# cat 14b
      {
          "schemaVersion": 2,
          "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
          "config": {
              "mediaType": "application/vnd.docker.container.image.v1+json",
              "digest": "sha256:78b3b822087d5199783c8203553a5a92ce5eb7b683a5a81003f8efea9b399d74",
              "size": 488
          },
          "layers": [
              {
                  "mediaType": "application/vnd.ollama.image.model",
                  "digest": "sha256:a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e",
                  "size": 9276184896
              },
              {
                  "mediaType": "application/vnd.ollama.image.template",
                  "digest": "sha256:ae370d884f108d16e7cc8fd5259ebc5773a0afa6e078b11f4ed7e39a27e0dfc4",
                  "size": 1723
              },
              {
                  "mediaType": "application/vnd.ollama.image.license",
                  "digest": "sha256:d18a5cc71b84bc4af394a31116bd3932b42241de70c77d2b76d69a314ec8aa12",
                  "size": 11338
              },
              {
                  "mediaType": "application/vnd.ollama.image.params",
                  "digest": "sha256:cff3f395ef3756ab63e58b0ad1b32bb6f802905cae1472e6a12034e4246fbbdb",
                  "size": 120
              }
          ]
      }

      # 大模型分析
      # 1. 进入模型的 Docker 镜像层目录
      cd /root/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e

      # sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e 是一个 ​​二进制 blob 文件​​（非目录），无法直接用 cd 进入。这是 Ollama 存储模型权重的底层文件格式。

      # 获取模型的本地路径和配置文件位置
      > ollama show qwen3:14b

      root@dcf9a775ca0e:~/.ollama/models/blobs# ollama show qwen3:14b
      Model
        architecture        qwen3
        parameters          14.8B
        context length      40960
        embedding length    5120
        quantization        Q4_K_M

      Capabilities
        completion
        tools
        thinking

      Parameters
        stop              "<|im_start|>"
        stop              "<|im_end|>"
        temperature       0.6
        top_k             20
        top_p             0.95
        repeat_penalty    1

      License
        Apache License
        Version 2.0, January 2004
        ...

    root@dcf9a775ca0e:~/.ollama/models/blobs#
    ```

  + 更新 Docker 容器 时区, 时间设置
    ```sh
      docker cp /etc/localtime ollama_container_GPU:/etc/localtime
      # 部分镜像需要
      docker cp /etc/timezone ollama_container_GPU:/etc/timezone
    ```

  + digitalman_container 容器, 添加 test.py 测试时区 时间 输出
    ```python
      docker cp ./test.py digitalman_container:/app/src/test.py

      # 更新 Agent_1_2.py
      docker cp ./Agent_1_2.py digitalman_container:/app/src/LLMAgent/Agent_1_2.py
    ```

# `上学 和 学习` 是两件事情

# 2025年6月28日 周六
  + 修改 modelfile.yaml
    ```bash
      # Ollama 容器执行 创建命令
      > ollama create qwen3_nothink:14b -f ./modelfile.yaml
      gathering model components
      copying file sha256:a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e 100%
      parsing GGUF
      using existing layer sha256:a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e
      creating new layer sha256:a866183fb84cbf0efb694c9e47bb50b2c634f2acdd7ef36d24ab86606f62826c
      using existing layer sha256:d18a5cc71b84bc4af394a31116bd3932b42241de70c77d2b76d69a314ec8aa12
      using existing layer sha256:cff3f395ef3756ab63e58b0ad1b32bb6f802905cae1472e6a12034e4246fbbdb
      writing manifest
      success
    ```

  + 对话 qwen3_nothink:14b
    虽然已经关闭思考，但是 仍然会 输出`<think>`标签。。。
    ```bash
      >>> 你好
        <think>

        </think>

        nihao! 怎么样，有什么我可以帮你的吗？
    ```

    + 修改 modelfile.yaml
      ```yaml
        """
        # 47 行修改(从修改内容来看，我感觉是 去除 /no_think 标识的。不确定是否 有用。。。先 备份在此处)
        {{ if hasPrefix .Content "/no_think" }}{{ trimPrefix .Content "/no_think" }}{{ else }}{{ .Content }}{{ end }}<|im_end|>

        # 49行 未修改
        {{ if .Content }}{{ .Content }}
        # 49行 已修改 修改(感觉这里像是 去除 think 标签)
        {{ if .Content }}{{ replace .Content "<think>" "" }}{{ replace .Content "</think>" "" }}
        """
      ```

  + 重复创建 同名模型会报错，解决方案
    ```
      # 方法一：删除旧模型后重建
      ollama rm qwen3_nothink:14b
      ollama create qwen3_nothink:14b -f ./modelfile.yaml

      # 方法二：覆盖创建（需 Ollama v0.1.30+）
      ollama create qwen3_nothink:14b -f ./modelfile.yaml --force
    ```

  + Cursor 优化 agent, 增加代理中间件
    过滤 think 标签

# 2025年6月30日 周一
  + TTS 正则优化
    + 尝试使用 NLP开源库

  + AI智能体:
    1. 提示词优化 增加角色名称 暂定为 `交通管理小助手`
    2. action 失败后，捕获错误 返回 失败原因，通知给数字人前端
    3. 最大迭代限制次数设置为3
    4. 优化 logger info 逻辑
    5. 交通态势统计接口 /api/trafficSituation 增加判空逻辑

  + TTS 分词解析
    增加 年月日、时分秒 解析逻辑。优化 语音播报口语

  + 更新数字人视频
    ```bash
      > docker cp ./open_mouth.mp4 digitalman_container:/app/public/video/open_mouth.mp4
      > docker cp ./open_mouth1.mp4 digitalman_container:/app/public/video/open_mouth1.mp4
      > docker cp ./open_mouth2.mp4 digitalman_container:/app/public/video/open_mouth2.mp4
      > docker cp ./shut_up.mp4 digitalman_container:/app/public/video/shut_up.mp4

      # 更新 AIChat.js
      > docker cp ./AIChat.js digitalman_container:/app/public/js/AIChat.js
    ```

  + 研究 docker 容器，添加 多显卡
   **Ollama GPU 模式**
    ```bash
      > docker run -d --name ollama_container_GPU --restart unless-stopped -p 11434:11434 --gpus all -e NVIDIA_VISIBLE_DEVICES=0 -e OLLAMA_DEVICE=cuda ollama_qwen3_32b_container
      // -v /opt/ai/ollama:/root/.ollama \  # 挂载模型存储目录 * 可选配置 不需要
    ```

  + docker 镜像 + 容器
  ```
  os@os-S2600WFT:~/SSh_Docker_DEV/devcontainer/src/digitalman/digitalManVideo$ docker images
  REPOSITORY                   TAG                       IMAGE ID       CREATED         SIZE
  ollama_qwen3_32b_gpu         V1.0                      47e6ef946c79   4 days ago      41.6GB
  digitalman                   V1.0                      9ae14b7a3e9c   4 days ago      2.28GB
  stt_tts_server               V1.0                      a694800732f8   4 days ago      15.2GB
  syq163_emoti_voice           latest                    8b8466a48b2f   2 weeks ago     8.37GB
  digitalman_v2                latest                    b505258a7e01   2 weeks ago     1.78GB
  digitalman                   latest                    98e11966c2da   2 weeks ago     2.22GB
  <none>                       <none>                    9b3a8568be54   2 weeks ago     1.28GB
  stt_tts_serve                v1                        552b3992f9fa   2 weeks ago     13.4GB
  ollama_qwen3_32b_container   latest                    e7c50030d6a6   2 weeks ago     23.7GB
  emoti-voice                  v2                        5417b338b259   3 weeks ago     13.3GB
  nvidia/cuda                  11.8.0-base-ubuntu22.04   1e75b7decac0   19 months ago   239MB
  os@os-S2600WFT:~/SSh_Docker_DEV/devcontainer/src/digitalman/digitalManVideo$ docker ps -a
  CONTAINER ID   IMAGE                        COMMAND                   CREATED       STATUS                     PORTS                                                   NAMES
  0afd3cf00d18   stt_tts_serve:v1             "python testV2.py"        2 weeks ago   Exited (0) 2 weeks ago                                                             STT_TTS_container_v2
  6f75266c3ebf   syq163_emoti_voice           "/bin/sh -c 'streaml…"   2 weeks ago   Exited (137) 2 weeks ago                                                           recursing_diffie
  4ad9a0b3b9e8   digitalman                   "python ./main.py"        2 weeks ago   Up About an hour           0.0.0.0:9880->9880/tcp, [::]:9880->9880/tcp             digitalman_container
  fbbe6318a1e3   stt_tts_serve:v1             "python testV2.py"        2 weeks ago   Up About an hour           0.0.0.0:6005->6005/tcp, [::]:6005->6005/tcp, 8000/tcp   STT_TTS_container
  dcf9a775ca0e   ollama_qwen3_32b_container   "/bin/ollama serve"       2 weeks ago   Up 3 hours                 0.0.0.0:11434->11434/tcp, [::]:11434->11434/tcp         ollama_container_GPU
  ```

#
